{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e7ec65-d2e4-4a64-b7ba-148c0b06a2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T01:30:45.299394Z",
     "iopub.status.busy": "2025-09-21T01:30:45.298742Z",
     "iopub.status.idle": "2025-09-21T01:30:48.024064Z",
     "shell.execute_reply": "2025-09-21T01:30:48.023121Z",
     "shell.execute_reply.started": "2025-09-21T01:30:45.299146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from s3://fake-review-dataset-penguining/train/train.csv...\n",
      "Loading testing data from s3://fake-review-dataset-penguining/test/test.csv...\n",
      "\n",
      "Columns in your training file are: ['Is_fake', 'Review_text']\n",
      "Columns in your testing file are: ['Is_fake', 'Review_text']\n",
      "\n",
      "Training and testing data loaded successfully.\n",
      "\n",
      "Training the model...\n",
      "Model training complete!\n",
      "\n",
      "Evaluating model performance on the test set...\n",
      "Model Accuracy: 0.9420\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real (0)       0.98      0.95      0.96      3177\n",
      "    Fake (1)       0.78      0.92      0.85       666\n",
      "\n",
      "    accuracy                           0.94      3843\n",
      "   macro avg       0.88      0.93      0.90      3843\n",
      "weighted avg       0.95      0.94      0.94      3843\n",
      "\n",
      "--------------------------------------------------\n",
      "Success! Trained model has been saved as 'model_pipeline.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Install and Import Libraries\n",
    "# =======================================================================\n",
    "!pip install scikit-learn pandas nltk --upgrade --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- Download stopwords package ---\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# =======================================================================\n",
    "# Dataset Configuration\n",
    "# =======================================================================\n",
    "bucket = \"fake-review-dataset-penguining\"\n",
    "train_file_path = \"train/train.csv\"\n",
    "test_file_path = \"test/test.csv\"\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Load Train and Test Datasets\n",
    "# =======================================================================\n",
    "s3_train_path = f\"s3://{bucket}/{train_file_path}\"\n",
    "s3_test_path = f\"s3://{bucket}/{test_file_path}\"\n",
    "\n",
    "try:\n",
    "    print(f\"Loading training data from {s3_train_path}...\")\n",
    "    train_df = pd.read_csv(s3_train_path)\n",
    "\n",
    "    print(f\"Loading testing data from {s3_test_path}...\")\n",
    "    test_df = pd.read_csv(s3_test_path)\n",
    "\n",
    "    # --- DIAGNOSTIC STEP ---\n",
    "    print(f\"\\nColumns in your training file are: {train_df.columns.tolist()}\")\n",
    "    print(f\"Columns in your testing file are: {test_df.columns.tolist()}\")\n",
    "\n",
    "    text_column_name = 'Review_text'\n",
    "    label_column_name = 'Is_fake'\n",
    "\n",
    "    # Clean and prepare both dataframes\n",
    "    train_df = train_df.dropna(subset=[text_column_name, label_column_name])\n",
    "    train_df[label_column_name] = train_df[label_column_name].astype(int)\n",
    "\n",
    "    test_df = test_df.dropna(subset=[text_column_name, label_column_name])\n",
    "    test_df[label_column_name] = test_df[label_column_name].astype(int)\n",
    "\n",
    "    print(\"\\nTraining and testing data loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nFATAL ERROR: File not found. The path '{e.filename}' is incorrect.\")\n",
    "    print(\"Please check your bucket name and file paths in the configuration section.\")\n",
    "    sys.exit()\n",
    "except KeyError:\n",
    "    print(f\"\\nFATAL ERROR: A required column was not found.\")\n",
    "    print(\"Please check the 'Columns in your ... file' output above and correct the column names in the script.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Define Features, Preprocess, and Train the Model\n",
    "# =======================================================================\n",
    "# Assign the data directly from files\n",
    "X_train = train_df[text_column_name]\n",
    "y_train = train_df[label_column_name]\n",
    "X_test = test_df[text_column_name]\n",
    "y_test = test_df[label_column_name]\n",
    "\n",
    "# --- Create a Combined Stop Words List for English and Malay ---\n",
    "english_stop_words = list(stopwords.words('english'))\n",
    "malay_stop_words = [\n",
    "    \"ada\", \"adalah\", \"akan\", \"aku\", \"anda\", \"apa\", \"atau\", \"bahawa\", \"banyak\", \"dan\", \"dengan\",\n",
    "    \"di\", \"dia\", \"ini\", \"itu\", \"jadi\", \"jika\", \"juga\", \"kamu\", \"kami\", \"ke\", \"kepada\", \"kerana\",\n",
    "    \"ketika\", \"kita\", \"lagi\", \"lain\", \"maka\", \"mereka\", \"pada\", \"pula\", \"saja\", \"saya\", \"seperti\",\n",
    "    \"sudah\", \"telah\", \"tetapi\", \"tidak\", \"untuk\", \"yang\"\n",
    "]\n",
    "combined_stop_words = english_stop_words + malay_stop_words\n",
    "\n",
    "# --- Create the scikit-learn pipeline ---\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(max_features=5000, stop_words=combined_stop_words)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# --- Train the model on the training data ---\n",
    "print(\"\\nTraining the model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Evaluate the Model and Save the Final Artifact\n",
    "# =======================================================================\n",
    "# --- Evaluate the model on the unseen testing data ---\n",
    "print(\"\\nEvaluating model performance on the test set...\")\n",
    "predictions = model_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=['Real (0)', 'Fake (1)']))\n",
    "\n",
    "# --- Save the final trained model pipeline ---\n",
    "joblib.dump(model_pipeline, 'model_pipeline.joblib')\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Success! Trained model has been saved as 'model_pipeline.joblib'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325022de-e006-48a2-886f-01c279ec29df",
   "metadata": {},
   "source": [
    "## User activity model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54c439d-8401-424e-96c0-43ad5137b22e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T04:14:41.685257Z",
     "iopub.status.busy": "2025-09-21T04:14:41.684944Z",
     "iopub.status.idle": "2025-09-21T04:14:43.817025Z",
     "shell.execute_reply": "2025-09-21T04:14:43.816321Z",
     "shell.execute_reply.started": "2025-09-21T04:14:41.685231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from s3://fake-review-dataset-penguining/train/useractivity_train.csv...\n",
      "Loading testing data from s3://fake-review-dataset-penguining/test/useractivity_test.csv...\n",
      "\n",
      "✅ Data loaded successfully!\n",
      "Train shape: (13116, 16)\n",
      "Test shape: (3280, 16)\n",
      "✅ Combined feature matrix shapes:\n",
      "Train: (13116, 5004)\n",
      "Test: (3280, 5004)\n",
      "\n",
      "Clustering training data...\n",
      "\n",
      "✅ Clustering complete!\n",
      "    reviewer_id                                        review_text  rating  \\\n",
      "0  1.040000e+20  Recommended untuk bersantai di pagi hari. Roti...     5.0   \n",
      "1  1.080000e+20  Senang parking... Bersebelahan dengan public b...     5.0   \n",
      "2  1.120000e+20  We had an amazing lunch at here and the staff ...     5.0   \n",
      "3  1.050000e+20  The BEST eggs I've ever eaten in a hotel resta...     5.0   \n",
      "4  1.060000e+20                                  Best place to eat     4.0   \n",
      "5  1.170000e+20                                  Beli kek kat sini     5.0   \n",
      "6  1.130000e+20  It was great. The staff was excellent and very...     5.0   \n",
      "7  1.000000e+20  Kali tuk tek asin gilak cerik ku lamb chop nya...     4.0   \n",
      "8  1.070000e+20  Super delicious vegetarian food!\\nReally kind ...     5.0   \n",
      "9  1.020000e+20  Memang awesome. Byk pilihan mee dan tpt sgt cool.     5.0   \n",
      "\n",
      "   total_number_of_reviews_by_reviewer  is_local_guide  reviews_per_day  \\\n",
      "0                                    4             0.0         0.191326   \n",
      "1                                   52             1.0         0.217610   \n",
      "2                                   42             1.0         0.266851   \n",
      "3                                   15             0.0         0.261821   \n",
      "4                                   24             1.0         0.210769   \n",
      "5                                   17             1.0         0.209080   \n",
      "6                                   10             0.0         0.236886   \n",
      "7                                   11             1.0         0.118243   \n",
      "8                                  104             1.0         0.238731   \n",
      "9                                    1             0.0         0.244548   \n",
      "\n",
      "   cluster  \n",
      "0        2  \n",
      "1        1  \n",
      "2        1  \n",
      "3        2  \n",
      "4        1  \n",
      "5        1  \n",
      "6        2  \n",
      "7        0  \n",
      "8        1  \n",
      "9        2  \n",
      "\n",
      "Clustered datasets saved:\n",
      "Train → train/useractivity_train_clustered.csv\n",
      "Test  → test/useractivity_test_clustered.csv\n",
      "\n",
      "📊 Cluster Summary:\n",
      "   cluster  avg_rating  avg_total_reviews  pct_local_guides  \\\n",
      "0        0    3.851240          60.418733          0.550964   \n",
      "1        1    4.540908          76.533084          1.000000   \n",
      "2        2    4.808672           4.926393          0.000000   \n",
      "3        3    4.062756         634.978172          0.990450   \n",
      "4        4    1.249310          19.174793          0.327047   \n",
      "\n",
      "   avg_reviews_per_day  cluster_size  \n",
      "0             0.112380           363  \n",
      "1             0.228431          5879  \n",
      "2             0.227067          3967  \n",
      "3             0.227931           733  \n",
      "4             0.226413          2174  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# --- Download stopwords package ---\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# =======================================================================\n",
    "# Dataset Configuration\n",
    "# =======================================================================\n",
    "bucket = \"fake-review-dataset-penguining\"\n",
    "train_file_path = \"train/useractivity_train.csv\"\n",
    "test_file_path = \"test/useractivity_test.csv\"\n",
    "\n",
    "s3_train_path = f\"s3://{bucket}/{train_file_path}\"\n",
    "s3_test_path = f\"s3://{bucket}/{test_file_path}\"\n",
    "\n",
    "# =======================================================================\n",
    "# Load Data\n",
    "# =======================================================================\n",
    "print(f\"Loading training data from {s3_train_path}...\")\n",
    "train_df = pd.read_csv(s3_train_path)\n",
    "\n",
    "print(f\"Loading testing data from {s3_test_path}...\")\n",
    "test_df = pd.read_csv(s3_test_path)\n",
    "\n",
    "print(\"\\n✅ Data loaded successfully!\")\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# =======================================================================\n",
    "# Text Preprocessing\n",
    "# =======================================================================\n",
    "english_stop_words = list(stopwords.words('english'))\n",
    "malay_stop_words = [\n",
    "    \"ada\",\"adalah\",\"akan\",\"aku\",\"anda\",\"apa\",\"atau\",\"bahawa\",\"banyak\",\"dan\",\"dengan\",\n",
    "    \"di\",\"dia\",\"ini\",\"itu\",\"jadi\",\"jika\",\"juga\",\"kamu\",\"kami\",\"ke\",\"kepada\",\"kerana\",\n",
    "    \"ketika\",\"kita\",\"lagi\",\"lain\",\"maka\",\"mereka\",\"pada\",\"pula\",\"saja\",\"saya\",\"seperti\",\n",
    "    \"sudah\",\"telah\",\"tetapi\",\"tidak\",\"untuk\",\"yang\"\n",
    "]\n",
    "combined_stop_words = english_stop_words + malay_stop_words\n",
    "\n",
    "text_column = \"review_text\"\n",
    "train_df = train_df.dropna(subset=[text_column])\n",
    "test_df = test_df.dropna(subset=[text_column])\n",
    "\n",
    "# =======================================================================\n",
    "# Feature Engineering (numeric)\n",
    "# =======================================================================\n",
    "# Convert published_at_date to datetime\n",
    "for df in [train_df, test_df]:\n",
    "    df[\"published_at_date\"] = pd.to_datetime(df[\"published_at_date\"], errors=\"coerce\")\n",
    "\n",
    "# Compute review frequency per reviewer (reviews/day)\n",
    "def compute_review_frequency(df):\n",
    "    reviewer_stats = df.groupby(\"reviewer_id\")[\"published_at_date\"].agg([\"min\", \"max\", \"count\"])\n",
    "    reviewer_stats[\"active_days\"] = (reviewer_stats[\"max\"] - reviewer_stats[\"min\"]).dt.days + 1\n",
    "    reviewer_stats[\"reviews_per_day\"] = reviewer_stats[\"count\"] / reviewer_stats[\"active_days\"].replace(0, 1)\n",
    "    return reviewer_stats[\"reviews_per_day\"]\n",
    "\n",
    "train_df = train_df.merge(compute_review_frequency(train_df), on=\"reviewer_id\", how=\"left\")\n",
    "test_df = test_df.merge(compute_review_frequency(test_df), on=\"reviewer_id\", how=\"left\")\n",
    "\n",
    "# Replace missing numeric with 0\n",
    "numeric_features = [\"rating\", \"total_number_of_reviews_by_reviewer\", \"is_local_guide\", \"reviews_per_day\"]\n",
    "for col in numeric_features:\n",
    "    train_df[col] = train_df[col].fillna(0)\n",
    "    test_df[col] = test_df[col].fillna(0)\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(train_df[numeric_features])\n",
    "X_test_num = scaler.transform(test_df[numeric_features])\n",
    "\n",
    "# =======================================================================\n",
    "# TF-IDF Vectorization\n",
    "# =======================================================================\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=combined_stop_words)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df[text_column])\n",
    "X_test_tfidf = vectorizer.transform(test_df[text_column])\n",
    "\n",
    "# =======================================================================\n",
    "# Combine Text + Numeric Features\n",
    "# =======================================================================\n",
    "X_train = hstack([X_train_tfidf, X_train_num])\n",
    "X_test = hstack([X_test_tfidf, X_test_num])\n",
    "\n",
    "print(\"✅ Combined feature matrix shapes:\")\n",
    "print(\"Train:\", X_train.shape)\n",
    "print(\"Test:\", X_test.shape)\n",
    "\n",
    "# =======================================================================\n",
    "# KMeans Clustering\n",
    "# =======================================================================\n",
    "n_clusters = 5  # tune this later\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "\n",
    "print(\"\\nClustering training data...\")\n",
    "train_clusters = kmeans.fit_predict(X_train)\n",
    "test_clusters = kmeans.predict(X_test)\n",
    "\n",
    "# Attach results\n",
    "train_df[\"cluster\"] = train_clusters\n",
    "test_df[\"cluster\"] = test_clusters\n",
    "\n",
    "print(\"\\n✅ Clustering complete!\")\n",
    "print(train_df[[\"reviewer_id\",\"review_text\",\"rating\",\"total_number_of_reviews_by_reviewer\",\"is_local_guide\",\"reviews_per_day\",\"cluster\"]].head(10))\n",
    "\n",
    "# =======================================================================\n",
    "# Save Results\n",
    "# =======================================================================\n",
    "output_train = \"train/useractivity_train_clustered.csv\"\n",
    "output_test = \"test/useractivity_test_clustered.csv\"\n",
    "\n",
    "train_df.to_csv(f\"s3://{bucket}/{output_train}\", index=False)\n",
    "test_df.to_csv(f\"s3://{bucket}/{output_test}\", index=False)\n",
    "\n",
    "print(\"\\nClustered datasets saved:\")\n",
    "print(\"Train →\", output_train)\n",
    "print(\"Test  →\", output_test)\n",
    "\n",
    "# =======================================================================\n",
    "# Cluster Profiling / Summary\n",
    "# =======================================================================\n",
    "cluster_summary = train_df.groupby(\"cluster\").agg({\n",
    "    \"rating\": \"mean\",\n",
    "    \"total_number_of_reviews_by_reviewer\": \"mean\",\n",
    "    \"is_local_guide\": \"mean\",  # proportion of local guides\n",
    "    \"reviews_per_day\": \"mean\",\n",
    "    \"review_id\": \"count\"       # cluster size\n",
    "}).rename(columns={\n",
    "    \"rating\": \"avg_rating\",\n",
    "    \"total_number_of_reviews_by_reviewer\": \"avg_total_reviews\",\n",
    "    \"is_local_guide\": \"pct_local_guides\",\n",
    "    \"reviews_per_day\": \"avg_reviews_per_day\",\n",
    "    \"review_id\": \"cluster_size\"\n",
    "}).reset_index()\n",
    "\n",
    "print(\"\\n📊 Cluster Summary:\")\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ad4ca48-0272-4f8b-8113-11a642a443b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T06:17:21.761217Z",
     "iopub.status.busy": "2025-09-21T06:17:21.760943Z",
     "iopub.status.idle": "2025-09-21T06:17:22.826932Z",
     "shell.execute_reply": "2025-09-21T06:17:22.826418Z",
     "shell.execute_reply.started": "2025-09-21T06:17:21.761194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔑 Top Keywords Per Cluster:\n",
      "Cluster 0: food, good, sedap, nice, makanan, place, shi, nasi, taste, harga\n",
      "Cluster 1: food, good, sedap, nice, nasi, place, delicious, makanan, great, ok\n",
      "Cluster 2: good, sedap, food, service, nice, makanan, best, delicious, terbaik, great\n",
      "Cluster 3: food, good, nice, ok, delicious, de, shi, sedap, place, price\n",
      "Cluster 4: tak, order, service, food, makanan, bad, nasi, lambat, dah, sampai\n",
      "\n",
      "🚩 Suspicious accounts flagged and saved:\n",
      "Train → train/useractivity_train_flagged.csv\n",
      "\n",
      "Examples of suspicious reviewers:\n",
      "     reviewer_id account_creation_date  reviews_per_day  \\\n",
      "12  1.180000e+20   2015-07-09 13:31:14         0.180623   \n",
      "16  1.130000e+20   2017-07-11 05:40:06         0.236886   \n",
      "22  1.150000e+20   2017-01-03 08:20:01         0.229415   \n",
      "26  1.040000e+20   2015-09-30 06:41:48         0.191326   \n",
      "29  1.090000e+20   2017-09-03 16:36:26         0.266757   \n",
      "39  1.180000e+20   2015-07-09 13:31:14         0.180623   \n",
      "43  1.140000e+20   2017-02-18 12:27:33         0.215880   \n",
      "44  1.130000e+20   2017-07-11 05:40:06         0.236886   \n",
      "56  1.180000e+20   2015-07-09 13:31:14         0.180623   \n",
      "65  1.170000e+20   2016-11-08 11:50:23         0.209080   \n",
      "\n",
      "    total_number_of_reviews_by_reviewer    suspicion_flag  \n",
      "12                                  235  too_many_reviews  \n",
      "16                                  239  too_many_reviews  \n",
      "22                                  275  too_many_reviews  \n",
      "26                                  321  too_many_reviews  \n",
      "29                                  474  too_many_reviews  \n",
      "39                                  524  too_many_reviews  \n",
      "43                                  459  too_many_reviews  \n",
      "44                                  949  too_many_reviews  \n",
      "56                                  509  too_many_reviews  \n",
      "65                                  493  too_many_reviews  \n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Extra Analysis: Account Creation Date & Reviewer Profiling\n",
    "# =======================================================================\n",
    "\n",
    "# 1. Get account creation date per reviewer (earliest review date)\n",
    "reviewer_creation = train_df.groupby(\"reviewer_id\")[\"published_at_date\"].min().reset_index()\n",
    "reviewer_creation = reviewer_creation.rename(columns={\"published_at_date\": \"account_creation_date\"})\n",
    "train_df = train_df.merge(reviewer_creation, on=\"reviewer_id\", how=\"left\")\n",
    "\n",
    "# 2. Extract top keywords per cluster\n",
    "def top_keywords_per_cluster(X_tfidf, labels, vectorizer, top_n=10):\n",
    "    \"\"\"Return top keywords for each cluster based on TF-IDF weights\"\"\"\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    cluster_keywords = {}\n",
    "    for cluster_id in sorted(set(labels)):\n",
    "        cluster_center = X_tfidf[labels == cluster_id].mean(axis=0)\n",
    "        top_indices = np.array(cluster_center).ravel().argsort()[-top_n:][::-1]\n",
    "        cluster_keywords[cluster_id] = [terms[i] for i in top_indices]\n",
    "    return cluster_keywords\n",
    "\n",
    "print(\"\\n🔑 Top Keywords Per Cluster:\")\n",
    "keywords = top_keywords_per_cluster(X_train_tfidf, train_clusters, vectorizer, top_n=10)\n",
    "for cluster_id, words in keywords.items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(words)}\")\n",
    "\n",
    "# 3. Flag suspicious reviewers\n",
    "# Thresholds: adjust as needed\n",
    "freq_threshold = 1.0   # > 1 review per day on average\n",
    "reviews_threshold = 200  # > 200 total reviews\n",
    "recent_threshold_days = 30  # account created less than 30 days ago\n",
    "\n",
    "latest_date = train_df[\"published_at_date\"].max()\n",
    "\n",
    "suspicious_flags = []\n",
    "for _, row in train_df.iterrows():\n",
    "    is_suspicious = False\n",
    "    reasons = []\n",
    "    \n",
    "    # Too frequent\n",
    "    if row[\"reviews_per_day\"] > freq_threshold:\n",
    "        is_suspicious = True\n",
    "        reasons.append(\"high_frequency\")\n",
    "        \n",
    "    # Too many total reviews\n",
    "    if row[\"total_number_of_reviews_by_reviewer\"] > reviews_threshold:\n",
    "        is_suspicious = True\n",
    "        reasons.append(\"too_many_reviews\")\n",
    "    \n",
    "    # Very new account\n",
    "    if (latest_date - row[\"account_creation_date\"]).days < recent_threshold_days:\n",
    "        is_suspicious = True\n",
    "        reasons.append(\"new_account\")\n",
    "    \n",
    "    suspicious_flags.append(\",\".join(reasons) if is_suspicious else \"normal\")\n",
    "\n",
    "train_df[\"suspicion_flag\"] = suspicious_flags\n",
    "\n",
    "# 4. Save updated file with suspicious flags\n",
    "output_train_flagged = \"train/useractivity_train_flagged.csv\"\n",
    "train_df.to_csv(f\"s3://{bucket}/{output_train_flagged}\", index=False)\n",
    "\n",
    "print(\"\\n🚩 Suspicious accounts flagged and saved:\")\n",
    "print(\"Train →\", output_train_flagged)\n",
    "\n",
    "# Show some flagged examples\n",
    "print(\"\\nExamples of suspicious reviewers:\")\n",
    "print(train_df[train_df[\"suspicion_flag\"] != \"normal\"][[\"reviewer_id\",\"account_creation_date\",\"reviews_per_day\",\"total_number_of_reviews_by_reviewer\",\"suspicion_flag\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6abeba9f-ea55-4d22-a8cf-fdf2cfb9baf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T06:25:25.099485Z",
     "iopub.status.busy": "2025-09-21T06:25:25.099176Z",
     "iopub.status.idle": "2025-09-21T06:25:25.111367Z",
     "shell.execute_reply": "2025-09-21T06:25:25.109341Z",
     "shell.execute_reply.started": "2025-09-21T06:25:25.099459Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_user_cluster(new_user, vectorizer, scaler, kmeans):\n",
    "    \"\"\"\n",
    "    new_user: dict with keys\n",
    "        - review_text\n",
    "        - rating\n",
    "        - total_number_of_reviews_by_reviewer\n",
    "        - is_local_guide\n",
    "        - published_at_date (datetime)\n",
    "        - reviewer_id\n",
    "    \"\"\"\n",
    "    # --- Preprocess text ---\n",
    "    text_vec = vectorizer.transform([new_user[\"review_text\"]])\n",
    "    \n",
    "    # --- Compute reviews_per_day ---\n",
    "    # assume new_user includes first review date for account_creation_date\n",
    "    active_days = max((new_user[\"latest_review_date\"] - new_user[\"account_creation_date\"]).days, 1)\n",
    "    reviews_per_day = new_user[\"total_number_of_reviews_by_reviewer\"] / active_days\n",
    "    \n",
    "    # --- Numeric features ---\n",
    "    num_features = np.array([[\n",
    "        new_user[\"rating\"],\n",
    "        new_user[\"total_number_of_reviews_by_reviewer\"],\n",
    "        new_user[\"is_local_guide\"],\n",
    "        reviews_per_day\n",
    "    ]])\n",
    "    num_scaled = scaler.transform(num_features)\n",
    "    \n",
    "    # --- Combine text + numeric ---\n",
    "    X_new = hstack([text_vec, num_scaled])\n",
    "    \n",
    "    # --- Predict cluster ---\n",
    "    cluster_id = kmeans.predict(X_new)[0]\n",
    "    \n",
    "    return cluster_id, reviews_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bca38470-0bb3-47b9-b2e9-510e9fef4edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T06:25:37.813443Z",
     "iopub.status.busy": "2025-09-21T06:25:37.812923Z",
     "iopub.status.idle": "2025-09-21T06:25:37.853537Z",
     "shell.execute_reply": "2025-09-21T06:25:37.850586Z",
     "shell.execute_reply.started": "2025-09-21T06:25:37.813411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User belongs to cluster 1\n",
      "Reviews per day: 3.00\n",
      "🚩 Suspicious activity detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Example random user\n",
    "new_user = {\n",
    "    \"review_text\": \"Great service, highly recommend!\",\n",
    "    \"rating\": 5,\n",
    "    \"total_number_of_reviews_by_reviewer\": 150,\n",
    "    \"is_local_guide\": 1,\n",
    "    \"account_creation_date\": datetime(2025, 8, 1),\n",
    "    \"latest_review_date\": datetime(2025, 9, 20),\n",
    "    \"reviewer_id\": \"random_123\"\n",
    "}\n",
    "\n",
    "cluster_id, reviews_per_day = predict_user_cluster(new_user, vectorizer, scaler, kmeans)\n",
    "\n",
    "print(f\"User belongs to cluster {cluster_id}\")\n",
    "print(f\"Reviews per day: {reviews_per_day:.2f}\")\n",
    "\n",
    "# Optionally check suspicion rules again\n",
    "if reviews_per_day > 1 or new_user[\"total_number_of_reviews_by_reviewer\"] > 200:\n",
    "    print(\"🚩 Suspicious activity detected\")\n",
    "else:\n",
    "    print(\"✅ Looks normal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2ce3566-3133-43f4-bb7e-8fe969d64790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T06:28:20.249888Z",
     "iopub.status.busy": "2025-09-21T06:28:20.247999Z",
     "iopub.status.idle": "2025-09-21T06:28:20.357642Z",
     "shell.execute_reply": "2025-09-21T06:28:20.356772Z",
     "shell.execute_reply.started": "2025-09-21T06:28:20.249851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(kmeans, \"kmeans_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
